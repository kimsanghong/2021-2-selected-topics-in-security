{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from pylab import rcParams\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model ,models, layers, optimizers, regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pickle\n",
    "import glob\n",
    "from tensorflow.python.keras.metrics import Metric\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "filenames = glob.glob(path + \"/data_SEQ_30_WIN*.csv\")  # data_SEQ_30_WIN_05.csv\n",
    "filenames.sort()\n",
    "print(filenames)\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsmt ae 에서 사용할 함수 \n",
    "# LSTM 모델은 (samples, timesteps, feature)에 해당하는 3d 차원의 shape을 가지므로, 데이터를 시퀀스 형태로 변환한다.\n",
    "def temporalize(X, y, timesteps): \n",
    "\toutput_X = []  \n",
    "\toutput_y = []\n",
    "\tfor i in range(len(X) - timesteps - 1):  \n",
    "\t\tt = []\n",
    "\t\tfor j in range(1, timesteps + 1):\n",
    "\t\t\t# Gather the past records upto the lookback period\n",
    "\t\t\tt.append(X[[(i + j + 1)], :])\n",
    "\t\toutput_X.append(t)\n",
    "\t\toutput_y.append(y[i + timesteps + 1])\n",
    "\treturn np.squeeze(np.array(output_X)), np.array(output_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content={}\n",
    "#데이터별로 이상치 index를 담아줄 딕셔너리. 마지막에 anomaly라고 라벨링된 index만 뽑아서 \"데이터이름\" : [1,2,4...] 이런식으로 key,value 값을 저장해줄것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#parameter = [0.0003, 0.0005, 0.001,0.005,0.01,0.03,0.05,0.1,0.3]\n",
    "parameter = [0.01 , 0.015, 0.02, 0.025, 0.03]\n",
    "\n",
    "for k in filenames:\n",
    "    for j in parameter:\n",
    "        print(j)\n",
    "        df = pd.read_csv(str(k))\n",
    "        \n",
    "        \n",
    "        # 1차 라벨링 부분\n",
    "        #isolationForest 적용 + 그림으로 보이기 ---------------------------------------------------------\n",
    "        \n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        clf=IsolationForest(n_estimators=50, max_samples=50, contamination = float(j), \n",
    "                                max_features=1.0, bootstrap=False, n_jobs=-1, random_state=None, verbose=0)\n",
    "        clf.fit(df)\n",
    "        pred = clf.predict(df)\n",
    "        df['anomaly'] = pred  # isolation forest로 1차 라벨링된것을 새피처로써 추가함.\n",
    "        outliers = df.loc[df['anomaly'] == -1]\n",
    "        outlier_index = list(outliers.index) # 1차 라벨링해서 이상치값의 index 만 outlier_index 리스트로 만듬. \n",
    "        pca = PCA(n_components = 5) # isolation 적용한후, 피처개수가 너무많아서 차원축소해준다. \n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(df)\n",
    "        X_reduce = pca.fit_transform(X)\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(X_reduce[:, 0], X_reduce[:, 1], s=4, lw=1, label=\"inliers\",c=\"green\")\n",
    "        ax.scatter(X_reduce[outlier_index,0],X_reduce[outlier_index,1],\n",
    "                   lw=1, s=4, c=\"red\", label=\"outliers\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "        print(\"isolation forest 적용후값  : \" + str(Counter(pred)))\n",
    "        \n",
    "         \n",
    "        # 2차 라벨링 부분 \n",
    "        # Train/valid data split -------------------------------------------------------------------------------\n",
    "        \n",
    "        input_x = X_reduce   # isolation forest 로 1차 라벨링한값을 차원축소한후 lstm ae 의 인풋데이터로 사용할것\n",
    "        input_y = []\n",
    "        # 1차 라벨링에서 1 정상 -1 비정상으로 1차 라벨링된값을 => 1 비정상 0 정상으로 숫자만 바꿔줌\n",
    "        for i in range(len(input_x)):   # 1 , -1\n",
    "            if i in outlier_index:\n",
    "                input_y.append(1) # 비정상\n",
    "            else :\n",
    "                input_y.append(0) # 정상\n",
    "        input_y = np.array(input_y)  \n",
    "        \n",
    "        timesteps = 3\n",
    "        x, y = temporalize(input_x, input_y, timesteps)   # LSTM 모델은 (samples, timesteps, feature)에 해당하는 3d 차원의 shape을 가지므로, 데이터를 시퀀스 형태로 변환한다.\n",
    "\n",
    "\n",
    "        # Split into train and valid\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2)\n",
    "        n_features = input_x.shape[1]\n",
    "        \n",
    "        # For training the autoencoder, split 0 / 1\n",
    "        x_train_y0 = x_train[y_train == 0] # 정상데이터 y0\n",
    "        x_train_y1 = x_train[y_train == 1] # 비정상데이터 y1\n",
    "        x_valid_y0 = x_valid[y_valid == 0]\n",
    "        x_valid_y1 = x_valid[y_valid == 1]\n",
    "\n",
    "        x_y0 = x[y==0] # 정상데이터 y0\n",
    "        x_y1 = x[y==1] # 비정상데이터 y1\n",
    "        \n",
    "        def flatten(X):\n",
    "            flattened_X = np.empty((X.shape[0], X.shape[2]))  # sample x features array.\n",
    "            for i in range(X.shape[0]):\n",
    "                flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "            return(flattened_X)\n",
    "        def scale(X, scaler):\n",
    "            for i in range(X.shape[0]):\n",
    "                X[i, :, :] = scaler.transform(X[i, :, :])  \n",
    "            return X\n",
    "\n",
    "        scaler = StandardScaler().fit(flatten(x_train_y0))\n",
    "        x_train_y0_scaled = scale(x_train_y0, scaler)\n",
    "        x_valid_scaled = scale(x_valid, scaler)\n",
    "        x_valid_y0_scaled = scale(x_valid_y0, scaler)\n",
    "        x_total_scaled = scale(x, scaler)   \n",
    "        x_y0_sacled = scale(x_y0, scaler)\n",
    "\n",
    "        # LSTM Autoencoder 적용 -------------------------------------------------------------------------------\n",
    "        epochs = 5\n",
    "        batch = 128\n",
    "        lr = 0.001\n",
    "        lstm_ae = models.Sequential()\n",
    "        # Encoder\n",
    "        lstm_ae.add(layers.LSTM(32, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))\n",
    "        lstm_ae.add(layers.LSTM(16, activation='relu', return_sequences=False))\n",
    "        lstm_ae.add(layers.RepeatVector(timesteps))\n",
    "        # Decoder\n",
    "        lstm_ae.add(layers.LSTM(16, activation='relu', return_sequences=True))\n",
    "        lstm_ae.add(layers.LSTM(32, activation='relu', return_sequences=True))\n",
    "        lstm_ae.add(layers.TimeDistributed(layers.Dense(n_features)))\n",
    "        # compile\n",
    "        lstm_ae.compile(loss='mse', optimizer=optimizers.Adam(lr))\n",
    "        # fit\n",
    "        history = lstm_ae.fit(x_y0_sacled, x_y0_sacled,\n",
    "                             epochs=epochs, batch_size=batch,\n",
    "                             validation_data=(x_valid_y0_scaled, x_valid_y0_scaled))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # normal/anomaly 를 구분해주는 기준 threshold 를 구함  -------------------------------------------------------\n",
    "\n",
    "        valid_x_predictions = lstm_ae.predict(x_valid_scaled)\n",
    "        mse = np.mean(np.power(flatten(x_valid_scaled) - flatten(valid_x_predictions), 2), axis=1)\n",
    "\n",
    "        error_df = pd.DataFrame({'Reconstruction_error':mse, \n",
    "                                 'True_class':list(y_valid)})\n",
    "        precision_rt, recall_rt, threshold_rt = sklearn.metrics.precision_recall_curve(error_df['True_class'], error_df['Reconstruction_error'])\n",
    "        \n",
    "        \n",
    "        # precision/recall 값\n",
    "        index_cnt = [cnt for cnt, (p, r) in enumerate(zip(precision_rt, recall_rt)) if p==r][0]\n",
    "        #print('precision: ',precision_rt[index_cnt],', recall: ',recall_rt[index_cnt])\n",
    "\n",
    "        # threshold 값\n",
    "        threshold_fixed = threshold_rt[index_cnt]\n",
    "        #print('threshold: ',threshold_fixed)\n",
    "\n",
    "\n",
    "        # threshold 보다 낮으면 정상(normal), 높으면 이상(anomaly)으로 판단한다.\n",
    "        # 정밀도 ( precision) : 참이라고 예측한 데이터중 실제로 참인 데이터 (참 / 참인데참,거짓인데참)\n",
    "        # 제햔율 ( RECALL ) : 실제로 참인 데이터중에서 참이라고 예측한 데이터? (참/ 참인데참, 참인데 거짓))\n",
    "\n",
    "        \n",
    "        \n",
    "        # threshold 값을 기준으로 normal/anomlay 인지 라벨링해줌 ----------------------------------------------------------------\n",
    "        test_x_predictions = lstm_ae.predict(x_total_scaled) \n",
    "        mse = np.mean(np.power(flatten(x_total_scaled) - flatten(test_x_predictions), 2), axis=1)\n",
    "        error_df = pd.DataFrame({'Reconstruction_error': mse,\n",
    "                                 'True_class': y.tolist()})\n",
    "        groups = error_df.groupby('True_class')\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        for name, group in groups:\n",
    "            ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
    "                    label= \"Break\" if name == 1 else \"Normal\")\n",
    "        ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "        ax.legend()\n",
    "        plt.title(\"Reconstruction error for different classes\")\n",
    "        plt.ylabel(\"Reconstruction error\")\n",
    "        plt.xlabel(\"Data point index\")\n",
    "        plt.show();\n",
    "        \n",
    "        \n",
    "        # classification by threshold\n",
    "        pred_y = [1 if e > threshold_fixed else 0 for e in error_df['Reconstruction_error'].values]\n",
    "        LABELS = ['Normal', 'Break']\n",
    "        conf_matrix = sklearn.metrics.confusion_matrix(error_df['True_class'], pred_y)\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt='d')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted Class'); plt.ylabel('True Class')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # TN/FP/FN 구하기 ------------------------------------------------------------------------------------\n",
    "        # 여기서 true 값은 1차 라벨링값 predict 는 2차 라벨링값\n",
    "        \n",
    "        df_p = pd.DataFrame(pred_y, columns=['anomaly'])\n",
    "        df_t = pd.DataFrame(error_df['True_class'].tolist(), columns=['anomaly'])\n",
    "        \n",
    "        anomaly_list_p = df_p[df_p['anomaly']==1].index\n",
    "        anomaly_list_T = df_t[df_t['anomaly']==1].index\n",
    "        anomaly_list_p = (anomaly_list_p +2).to_list()\n",
    "        anomaly_list_T = (anomaly_list_T +2).to_list()\n",
    "\n",
    "        TN = list(set(anomaly_list_T) & set(anomaly_list_p)) \n",
    "        FP = list(set(anomaly_list_p) - set(anomaly_list_T))\n",
    "        FN = list(set(anomaly_list_T) - set(anomaly_list_p))\n",
    "        \n",
    "                          \n",
    "        # anomaly index 만 리스트로 만들어서 dictionary 에 value 로 저장. ------------------------------------------------------------------------------------------\n",
    "        isolationforest_parameter = j\n",
    "        str_isolationforest_parameter = str(isolationforest_parameter)[2:]\n",
    "        if(len(str_isolationforest_parameter) != 5  ):\n",
    "            for i in range(5 - len(str_isolationforest_parameter)):\n",
    "                str_isolationforest_parameter=str_isolationforest_parameter + '0'\n",
    "            \n",
    "        \n",
    "        \n",
    "        Save_dict_name = str(k)[7:20]\n",
    "        content[Save_dict_name +'_p_'+ str(str_isolationforest_parameter)] = TN+FP+FN\n",
    "        \n",
    "        \n",
    "        with open('anomaly_index.pickle','wb') as fw:\n",
    "            pickle.dump(content, fw)\n",
    "            \n",
    "        file = open(\"anomaly_index.pickle\",'rb')\n",
    "        content = pickle.load(file)\n",
    "        content.keys()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
