{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "universal-satin",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import tqdm\n",
    "\n",
    "table = pd.read_csv(\"pjdata/dataIN_SEQ_30_WIN_04.csv\")\n",
    "# table = pd.DataFrame(data=sequences, columns=col)\n",
    "\n",
    "max_n = 100\n",
    "interval = 10\n",
    "wss = []\n",
    "sis = []\n",
    "for k in tqdm.tqdm(range(10, max_n+1, interval)):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1234)\n",
    "    kmeans.fit(table.iloc[:,1:].values)\n",
    "    wss = np.append(wss, kmeans.inertia_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "line = ax.plot(range(10, max_n+1, interval), wss, 'ro--', label='SSE')\n",
    "ax.set_ylim(wss.min()*0.95, wss.max()*1.05)\n",
    "ax.set_xticks(range(10, max_n+1, interval))\n",
    "ax.set_xlabel('cluster num')\n",
    "ax.set_ylabel('SSE')\n",
    "labels = [l.get_label() for l in line]\n",
    "plt.legend(line, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-russell",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "inout_flg = \"\" # \"\"\n",
    "label_dicts = {}\n",
    "iterations = []\n",
    "for SEQ in (\"10\",\"20\",\"30\",\"40\",\"50\"):\n",
    "    for WIN in (\"03\",\"04\",\"05\",\"06\"):\n",
    "        for CLUSTER_NUM in (20,30,50,100):\n",
    "            iterations.append((SEQ, WIN, CLUSTER_NUM))\n",
    "            \n",
    "for SEQ, WIN, CLUSTER_NUM in tqdm.tqdm(iterations):\n",
    "    cur = f\"SEQ_{SEQ}_WIN_{WIN}_CLS_{str(CLUSTER_NUM)}\"\n",
    "    table = pd.read_csv(f\"pjdata/data{inout_flg}_SEQ_{SEQ}_WIN_{WIN}.csv\")\n",
    "    kmeans = KMeans(n_clusters=CLUSTER_NUM, random_state=1234)\n",
    "    kmeans.fit(table.iloc[:,1:].values)\n",
    "    table['cluster'] = kmeans.predict(table.iloc[:,1:])\n",
    "    result = table.groupby('cluster').actions.mean()\n",
    "\n",
    "    print(cur)\n",
    "    print(result[((result >= 1.1) & (result <= 1.9)) | (result >= 2.1)])\n",
    "    print()\n",
    "\n",
    "    label_lists = []\n",
    "    tmp = list(result[((result >= 1.1) & (result <= 1.9)) | (result >= 2.1)].index)\n",
    "    if len(tmp) > 0:\n",
    "        for cls in tmp:\n",
    "            label_lists += list(table[table.cluster == cls].index)\n",
    "    label_lists.sort()\n",
    "    label_dicts[cur] = label_lists\n",
    "\n",
    "with open(f\"pjdata/kmeans{inout_flg}_results.pickle\", \"wb\") as f:\n",
    "    pickle.dump(label_dicts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-pickup",
   "metadata": {},
   "source": [
    "# dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import distro\n",
    "import re\n",
    "import subprocess\n",
    "import tables\n",
    "from tempfile import NamedTemporaryFile\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.neighbors import kneighbors_graph \n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "np.seterr(invalid = 'ignore')\n",
    "warnings.filterwarnings('ignore', category = DeprecationWarning)\n",
    "__all__ = ['DBSCAN', 'load', 'shoot']\n",
    "\n",
    "def memory():\n",
    "    mem_info = {}\n",
    "    if distro.linux_distribution()[0]:\n",
    "        with open('/proc/meminfo') as file:\n",
    "            c = 0\n",
    "            for line in file:\n",
    "                lst = line.split()\n",
    "                if str(lst[0]) == 'MemTotal:':\n",
    "                    mem_info['total'] = int(lst[1])\n",
    "                elif str(lst[0]) in ('MemFree:', 'Buffers:', 'Cached:'):\n",
    "                    c += int(lst[1])\n",
    "            mem_info['free'] = c\n",
    "            mem_info['used'] = (mem_info['total']) - c\n",
    "    elif distro.mac_ver()[0]:\n",
    "        ps = subprocess.Popen(['ps', '-caxm', '-orss,comm'], stdout=subprocess.PIPE).communicate()[0]\n",
    "        vm = subprocess.Popen(['vm_stat'], stdout=subprocess.PIPE).communicate()[0]\n",
    "        \n",
    "        # Iterate processes\n",
    "        process_lines = ps.split('\\n')\n",
    "        sep = re.compile('[\\s]+')\n",
    "        rss_total = 0  # kB\n",
    "        for row in range(1, len(process_lines)):\n",
    "            row_text = process_lines[row].strip()\n",
    "            row_elements = sep.split(row_text)\n",
    "            try:\n",
    "                rss = float(row_elements[0]) * 1024\n",
    "            except:\n",
    "                rss = 0  # ignore...\n",
    "            rss_total += rss\n",
    "            \n",
    "        # Process vm_stat\n",
    "        vm_lines = vm.split('\\n')\n",
    "        sep = re.compile(':[\\s]+')\n",
    "        vm_stats = {}\n",
    "        for row in range(1, len(vm_lines) - 2):\n",
    "            row_text = vm_lines[row].strip()\n",
    "            row_elements = sep.split(row_text)\n",
    "            vm_stats[(row_elements[0])] = int(row_elements[1].strip('\\.')) * 4096\n",
    "\n",
    "        mem_info['total'] = rss_total\n",
    "        mem_info['used'] = vm_stats[\"Pages active\"]\n",
    "        mem_info['free'] = vm_stats[\"Pages free\"]\n",
    "    else:\n",
    "        raise('Unsupported Operating System.\\n')\n",
    "        exit(1)\n",
    "\n",
    "    return mem_info\n",
    "\n",
    "\n",
    "def get_chunk_size(N, n):\n",
    "    mem_free = memory()['free']\n",
    "    if mem_free > 60000000:\n",
    "        chunks_size = int(((mem_free - 10000000) * 1000) / (4 * n * N))\n",
    "        return chunks_size\n",
    "    elif mem_free > 40000000:\n",
    "        chunks_size = int(((mem_free - 7000000) * 1000) / (4 * n * N))\n",
    "        return chunks_size\n",
    "    elif mem_free > 14000000:\n",
    "        chunks_size = int(((mem_free - 2000000) * 1000) / (4 * n * N))\n",
    "        return chunks_size\n",
    "    elif mem_free > 8000000:\n",
    "        chunks_size = int(((mem_free - 1400000) * 1000) / (4 * n * N))\n",
    "        return chunks_size\n",
    "    elif mem_free > 2000000:\n",
    "        chunks_size = int(((mem_free - 900000) * 1000) / (4 * n * N))\n",
    "        return chunks_size\n",
    "    elif mem_free > 1000000:\n",
    "        chunks_size = int(((mem_free - 400000) * 1000) / (4 * n * N))\n",
    "        return chunks_size\n",
    "    else:\n",
    "        raise MemoryError(\"\\nERROR: DBSCAN_multiplex @ get_chunk_size:\\n\"\n",
    "                          \"this machine does not have enough free memory \"\n",
    "                          \"to perform the remaining computations.\\n\")\n",
    "\n",
    "\n",
    "def load(hdf5_file_name, data, minPts, eps = None, quantile = 50, subsamples_matrix = None, samples_weights = None, \n",
    "metric = 'minkowski', p = 2, verbose = True):\n",
    "    \n",
    "    data = np.array(data, copy = False)\n",
    "    if data.ndim > 2:\n",
    "        raise ValueError(\"\\nERROR: DBSCAN_multiplex @ load:\\n\" \n",
    "                         \"the data array is of dimension %d. Please provide a two-dimensional \"\n",
    "                         \"array instead.\\n\" % data.ndim)\n",
    "\n",
    "    if subsamples_matrix is None:\n",
    "        subsamples_matrix = np.arange(data.shape[0], dtype = int)\n",
    "        subsamples_matrix = subsamples_matrix.reshape(1, -1)\n",
    "    else:\n",
    "        subsamples_matrix = np.array(subsamples_matrix, copy = False)\n",
    "\n",
    "    if subsamples_matrix.ndim > 2:\n",
    "        raise ValueError(\"\\nERROR: DBSCAN_multiplex @ load:\\n\"\n",
    "                         \"the array of subsampled indices is of dimension %d. \"\n",
    "                         \"Please provide a two-dimensional array instead.\\n\" % subsamples_matrix.ndim)\n",
    "    if (data.dtype.char in np.typecodes['AllFloat'] and not np.isfinite(data.sum()) and not np.all(np.isfinite(data))):\n",
    "        raise ValueError('\\nERROR: DBSCAN_multiplex @ load:\\n'\n",
    "                         'the data vector contains at least one infinite or NaN entry.\\n')\n",
    "    if (subsamples_matrix.dtype.type is np.int_ and not np.isfinite(subsamples_matrix.sum()) and not np.all(np.isfinite(subsamples_matrix))):\n",
    "        raise ValueError('\\nERROR: DBSCAN_multiplex @ load:\\n' \n",
    "                         'the array of subsampled indices contains at least one infinite or NaN entry.\\n')\n",
    "    if not np.all(subsamples_matrix >= 0):\n",
    "        raise ValueError('\\nERROR: DBSCAN_multiplex @ load:\\n'\n",
    "                         'the sampled indices should all be positive integers.\\n') \n",
    "\n",
    "    N_samples = data.shape[0]\n",
    "    N_runs, N_subsamples = subsamples_matrix.shape\n",
    "\n",
    "    if N_subsamples > N_samples:\n",
    "        raise ValueError('\\nERROR: DBSCAN_multiplex @ load:\\n'\n",
    "                         'the number of sampled indices cannot exceed the total number of samples in the whole data-set.\\n')\n",
    "        \n",
    "    for i in range(N_runs):\n",
    "        subsamples_matrix[i] = np.unique(subsamples_matrix[i])\n",
    "        \n",
    "    if not isinstance(minPts, int):\n",
    "        raise TypeError(\"\\nERROR: DBSCAN_multiplex @ load:\\n\"\n",
    "                        \"the parameter 'minPts' must be an integer.\\n\")\n",
    "    if minPts < 2:\n",
    "        raise ValueError(\"\\nERROR: DBSCAN_multiplex @ load:\\n\"\n",
    "                         \"the value of 'minPts' must be larger than 1.\\n\")        \n",
    "\n",
    "    if eps is None:\n",
    "        if verbose:\n",
    "            print((\"INFO: DBSCAN_multiplex @ load:\\n\"\n",
    "                  \"starting the determination of an appropriate value of 'eps' for this data-set\"\n",
    "                  \" and for the other parameter of the DBSCAN algorithm set to {minPts}.\\n\"\n",
    "                  \"This might take a while.\".format(**locals())))\n",
    "\n",
    "        beg_eps = time.time()\n",
    "        quantile = np.rint(quantile)\n",
    "        quantile = np.clip(quantile, 0, 100)\n",
    "        k_distances = kneighbors_graph(data, minPts, mode = 'distance', metric = metric, p = p).data\n",
    " \n",
    "        radii = np.zeros(N_samples, dtype = float)\n",
    "        for i in range(0, minPts):\n",
    "            radii = np.maximum(radii, k_distances[i::minPts]) \n",
    "\n",
    "        if quantile == 50:     \n",
    "            eps = round(np.median(radii, overwrite_input = True), 4)\n",
    "        else:\n",
    "            eps = round(np.percentile(radii, quantile), 4)\n",
    "            \n",
    "        end_eps = time.time()\n",
    "        if verbose:\n",
    "            print((\"\\nINFO: DBSCAN_multiplex @ load:\\n\"\n",
    "                  \"done with evaluating parameter 'eps' from the data-set provided.\"\n",
    "                  \" This took {} seconds. Value of epsilon: {}.\".format(round(end_eps - beg_eps, 4), eps)))\n",
    "    else:\n",
    "        if not (isinstance(eps, float) or isinstance(eps, int)):\n",
    "            raise ValueError(\"\\nERROR: DBSCAN_multiplex @ load:\\n\"\n",
    "                             \"please provide a numeric value for the radius 'eps'.\\n\")\n",
    "        if not eps > 0.0:\n",
    "            raise ValueError(\"\\nERROR: DBSCAN_multiplex @ load:\\n\"\n",
    "                             \"the radius 'eps' must be positive.\\n\")\n",
    "        eps = round(eps, 4)\n",
    "\n",
    "    if verbose:\n",
    "        print((\"\\nINFO: DBSCAN_multiplex @ load:\\n\"\n",
    "             \"identifying the neighbors within an hypersphere of radius {eps} around each sample,\"\n",
    "             \" while at the same time evaluating the number of epsilon-neighbors for each sample.\\n\"\n",
    "             \"This might take a fair amount of time.\".format(**locals())))\n",
    "\n",
    "    beg_neigh = time.time()\n",
    "\n",
    "    fileh = tables.open_file(hdf5_file_name, mode = 'r+')\n",
    "    DBSCAN_group = fileh.create_group(fileh.root, 'DBSCAN_group')\n",
    "    neighborhoods_indices = fileh.create_earray(DBSCAN_group, 'neighborhoods_indices', tables.Int32Atom(), (0,), \n",
    "                                                'Indices array for sparse matrix of neighborhoods', \n",
    "                                                expectedrows = int((N_samples ** 2) / 50))\n",
    "\n",
    "    neighborhoods_indptr = np.zeros(1, dtype = np.int64)\n",
    "    neighbors_counts = fileh.create_carray(DBSCAN_group, 'neighbors_counts', tables.Int32Atom(), (N_runs, N_samples), \n",
    "                                           'Array of the number of neighbors around each sample of a set of subsampled points', \n",
    "                                           filters = None)   \n",
    "    chunks_size = get_chunk_size(N_samples, 3)\n",
    "    \n",
    "    for i in range(0, N_samples, chunks_size):\n",
    "        chunk = data[i:min(i + chunks_size, N_samples)]\n",
    "        D = pairwise_distances(chunk, data, metric = metric, p = p, n_jobs = 1)\n",
    "        D = (D <= eps)\n",
    "        if samples_weights is None:\n",
    "            for run in range(N_runs):\n",
    "                x = subsamples_matrix[run]\n",
    "                M = np.take(D, x, axis = 1)\n",
    "                legit_rows = np.intersect1d(i + np.arange(min(chunks_size, N_samples - i)), x, assume_unique = True)\n",
    "                M = np.take(M, legit_rows - i, axis = 0)\n",
    "                neighbors_counts[run, legit_rows] = M.sum(axis = 1)\n",
    "                del M\n",
    "        else:\n",
    "            for run in range(N_runs):\n",
    "                x = subsamples_matrix[run]\n",
    "                M = np.take(D, x, axis = 1)\n",
    "                legit_rows = np.intersect1d(i + np.arange(min(chunks_size, N_samples - i)), x, assume_unique = True)\n",
    "                M = np.take(M, legit_rows - i, axis = 0)\n",
    "                neighbors_counts[run, legit_rows] = np.array([np.sum(samples_weights[x[row]]) for row in M])\n",
    "                del M\n",
    "\n",
    "        candidates = np.where(D == True)\n",
    "        del D\n",
    "        neighborhoods_indices.append(candidates[1])\n",
    "        _, nbr = np.unique(candidates[0], return_counts = True)\n",
    "        counts = np.cumsum(nbr) + neighborhoods_indptr[-1]\n",
    "\n",
    "        del candidates\n",
    "        neighborhoods_indptr = np.append(neighborhoods_indptr, counts)\n",
    "\n",
    "    fileh.create_carray(DBSCAN_group, 'neighborhoods_indptr', tables.Int64Atom(), (N_samples + 1,), \n",
    "                        'Array of cumulative number of column indices for each row', filters = None)\n",
    "    fileh.root.DBSCAN_group.neighborhoods_indptr[:] = neighborhoods_indptr[:]\n",
    "    fileh.create_carray(DBSCAN_group, 'subsamples_matrix', tables.Int32Atom(), (N_runs, N_subsamples), \n",
    "                        'Array of subsamples indices', filters = None)\n",
    "    fileh.root.DBSCAN_group.subsamples_matrix[:] = subsamples_matrix[:]\n",
    "    fileh.close()\n",
    "\n",
    "    end_neigh = time.time()\n",
    "\n",
    "    if verbose:\n",
    "        print((\"\\nINFO: DBSCAN_multiplex @ load:\\n\"\n",
    "              \"done with the neighborhoods. This step took {} seconds.\".format(round(end_neigh - beg_neigh, 4))))\n",
    "    gc.collect()\n",
    "    \n",
    "    return eps\n",
    "\n",
    "def shoot(hdf5_file_name, minPts, sample_ID = 0, random_state = None, verbose = True):\n",
    "    fileh = tables.open_file(hdf5_file_name, mode = 'r+')\n",
    "\n",
    "    neighborhoods_indices = fileh.root.DBSCAN_group.neighborhoods_indices\n",
    "    neighborhoods_indptr = fileh.root.DBSCAN_group.neighborhoods_indptr[:]\n",
    "\n",
    "    neighbors_counts = fileh.root.DBSCAN_group.neighbors_counts[sample_ID]\n",
    "    subsampled_indices = fileh.root.DBSCAN_group.subsamples_matrix[sample_ID]\n",
    "\n",
    "    N_samples = neighborhoods_indptr.size - 1\n",
    "    N_runs, N_subsamples = fileh.root.DBSCAN_group.subsamples_matrix.shape\n",
    "\n",
    "    if not isinstance(sample_ID, int):\n",
    "        raise ValueError(\"\\nERROR: DBSCAN_multiplex @ shoot:\\n\"\n",
    "                         \"'sample_ID' must be an integer identifying the set of subsampled indices \"\n",
    "                         \"on which to perform DBSCAN clustering\\n\")    \n",
    "\n",
    "    if (sample_ID < 0) or (sample_ID >= N_runs):\n",
    "        raise ValueError(\"\\nERROR: DBSCAN_multiplex @ shoot:\\n\"\n",
    "                 \"'sample_ID' must belong to the interval [0; {}].\\n\".format(N_runs - 1))\n",
    "      \n",
    "    labels = np.full(N_samples, -2, dtype = int)\n",
    "    labels[subsampled_indices] = - 1\n",
    "    \n",
    "    random_state = check_random_state(random_state)\n",
    "    core_samples = np.flatnonzero(neighbors_counts >= minPts)\n",
    "    index_order = np.take(core_samples, random_state.permutation(core_samples.size))\n",
    "    cluster_ID = 0\n",
    "\n",
    "    for index in index_order:\n",
    "        if labels[index] not in {-1, -2}:\n",
    "            continue\n",
    "\n",
    "        labels[index] = cluster_ID\n",
    "        candidates = [index]\n",
    "        while len(candidates) > 0:\n",
    "            candidate_neighbors = np.zeros(0, dtype = np.int32)\n",
    "            for k in candidates:\n",
    "                candidate_neighbors = np.append(candidate_neighbors, neighborhoods_indices[neighborhoods_indptr[k]: neighborhoods_indptr[k+1]])\n",
    "                candidate_neighbors = np.unique(candidate_neighbors)\n",
    "            candidate_neighbors = np.intersect1d(candidate_neighbors, subsampled_indices, assume_unique = True)\n",
    "            not_noise_anymore = np.compress(np.take(labels, candidate_neighbors) == -1, candidate_neighbors)\n",
    "            labels[not_noise_anymore] = cluster_ID\n",
    "            candidates = np.intersect1d(not_noise_anymore, core_samples, assume_unique = True) \n",
    "        cluster_ID += 1\n",
    "\n",
    "    fileh.close()\n",
    "    gc.collect()\n",
    "    return core_samples, labels\n",
    "\n",
    "\n",
    "def DBSCAN(data, minPts, eps = None, quantile = 50, subsamples_matrix = None, samples_weights = None, \n",
    "metric = 'minkowski', p = 2, verbose = True):\n",
    "        \n",
    "    assert isinstance(minPts, int) or type(minPts) is np.int_\n",
    "    assert minPts > 1\n",
    "\n",
    "    if subsamples_matrix is None:\n",
    "        subsamples_matrix = np.arange(data.shape[0], dtype = int)\n",
    "        subsamples_matrix = subsamples_matrix.reshape(1, -1)\n",
    "    else:\n",
    "        subsamples_matrix = np.array(subsamples_matrix, copy = False)\n",
    "\n",
    "    N_runs = subsamples_matrix.shape[0]\n",
    "    N_samples = data.shape[0]\n",
    "\n",
    "    labels_matrix = np.zeros((N_runs, N_samples), dtype = int)\n",
    "\n",
    "    with NamedTemporaryFile('w', suffix = '.h5', delete = True, dir = './') as f:\n",
    "        eps = load(f.name, data, minPts, eps, quantile, subsamples_matrix, samples_weights, metric, p, verbose)\n",
    "\n",
    "        for run in range(N_runs):\n",
    "            _, labels = shoot(f.name, minPts, sample_ID = run, verbose = verbose)\n",
    "            labels_matrix[run] = labels\n",
    "\n",
    "    return eps, labels_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "inout_flg = \"IN\" # \"\"\n",
    "label_dicts = {}\n",
    "iterations = []\n",
    "for SEQ in (\"10\",\"20\",\"30\",\"40\",\"50\"):\n",
    "    for WIN in (\"03\",\"04\",\"05\",\"06\"):\n",
    "            iterations.append((SEQ, WIN))\n",
    "\n",
    "for SEQ, WIN in tqdm.tqdm(iterations):\n",
    "    table = pd.read_csv(f\"pjdata/data{inout_flg}_SEQ_{SEQ}_WIN_{WIN}.csv\")\n",
    "    SIZE = table.shape[0]\n",
    "    data = table.iloc[:SIZE,:]\n",
    "\n",
    "    N_iterations = 1\n",
    "    N_sub = data.shape[0]\n",
    "    subsamples_matrix = np.zeros((N_iterations, N_sub), dtype=int)\n",
    "\n",
    "    for i in range(N_iterations): \n",
    "        subsamples_matrix[i] = np.random.choice(data.shape[0], N_sub, replace=False)\n",
    "\n",
    "    eps, labels_matrix = DBSCAN(data.iloc[:,1:].values, minPts=5, subsamples_matrix=subsamples_matrix, verbose=False)\n",
    "\n",
    "    data['cluster'] = labels_matrix[0]\n",
    "    jn = (data[data.cluster != -1].groupby('cluster').actions.count() > 0).reset_index()\n",
    "    jn.columns = ['cluster', 'flg']\n",
    "    data = data.merge(jn, on='cluster', how='left').fillna(False)\n",
    "    result = data[data.flg==True].groupby('cluster').actions.mean()\n",
    "    print(cur)\n",
    "    print(data.cluster.value_counts())\n",
    "    print(result[((result >= 1.1) & (result <= 1.9)) | (result >= 2.1)])\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    label_lists = []\n",
    "    tmp = result[((result >= 1.1) & (result <= 1.9) | (result >= 2.1))].index\n",
    "    if len(tmp) > 0:\n",
    "        for cls in tmp:\n",
    "            label_lists += list(data[data.cluster == cls].index)\n",
    "    label_lists.sort()\n",
    "    label_dicts[cur] = label_lists\n",
    "\n",
    "with open(f\"pjdata/dbscan{inout_flg}_results.pickle\", \"wb\") as f:\n",
    "    pickle.dump(label_dicts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-antique",
   "metadata": {},
   "source": [
    "# DBSCAN - sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "iterations = []\n",
    "for SEQ in (\"30\",\"40\"):\n",
    "    for WIN in (\"03\",\"06\"):\n",
    "        for EPS in (2.0, 2.25, 2.5):\n",
    "            for MIN_SAMPLE in (5, 10):\n",
    "                iterations.append((SEQ, WIN, EPS, MIN_SAMPLE))\n",
    "\n",
    "for SEQ, WIN, EPS, MIN_SAMPLE in tqdm.tqdm(iterations):\n",
    "    cur = f\"SEQ_{SEQ}_WIN_{WIN}_MIN_{str(MIN_SAMPLE).zfill(2)}_EPS_{str(EPS)}\"\n",
    "    table = pd.read_csv(f\"pjdata/data_SEQ_{SEQ}_WIN_{WIN}.csv\")\n",
    "    dbscan = DBSCAN(eps=EPS, min_samples=MIN_SAMPLE)\n",
    "    dbscan.fit(table.iloc[:,1:].values)\n",
    "    table['cluster'] = dbscan.labels_\n",
    "    \n",
    "    print(cur)\n",
    "    print(table.cluster.value_counts())\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
